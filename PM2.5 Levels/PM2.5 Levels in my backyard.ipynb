{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2.5 - Local Conditions in Arapahoe County, Colorado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM2.5 refers to atmospheric particulate matter (PM) that have a diameter of less than 2.5 micrometers, which is about 3% the diameter of a human hair.\n",
    "\n",
    "Fine particles can come from various sources. They include power plants, motor vehicles, airplanes, residential wood burning, forest fires, agricultural burning, volcanic eruptions and dust storms.\n",
    "\n",
    "This increases the chances of humans and animals inhaling them into the bodies. Owing to their minute size, particles smaller than 2.5 micrometers are able to bypass the nose and throat and penetrate deep into the lungs and some may even enter the circulatory system. Studies have found a close link between exposure to fine particles and premature death from heart and lung disease. Fine particles are also known to trigger or worsen chronic disease such as asthma, heart attack, bronchitis and other respiratory problems.\n",
    "\n",
    "\n",
    "On a very clear and non-hazy day, the PM2.5 concentration can be as low as 5 μg/m3 or below. The 24-hour concentration of PM2.5 is considered unhealthy when it rises above 35.4 μg/m3.\n",
    "\n",
    "*Source:https://blissair.com/what-is-pm-2-5.htm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import datetime\n",
    "import itertools\n",
    "from pmdarima.arima import auto_arima\n",
    "%matplotlib inline\n",
    "pyplot.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed( 156464 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from Arapahoe County\n",
    "#### Using EPA's API by County for PM2.5 levels\n",
    "\n",
    "EPA.Gov has a wide range of resources that help track various air quality stats. EPA's API key is easy to obtain. Cannot load more than a year at a time. \n",
    "\n",
    "*Documentation for EPA's API: https://aqs.epa.gov/aqsweb/documents/data_api.html*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API inputs\n",
    "email = \" \" #insert email here\n",
    "key = \" \" #insert key here\n",
    "api_url = \"https://aqs.epa.gov/data/api/sampleData/byCounty\"\n",
    "\n",
    "json_list = []\n",
    "#data collection did not start for this area until 1999\n",
    "for year in range(1999,2020):\n",
    "    param = {'email': email, \n",
    "             'key': key,\n",
    "             'param':'88101', #code for the PM2.5 levels\n",
    "             'bdate':'%s0101' %year, \n",
    "             'edate':'%s1231' %year,\n",
    "             'state':'08', #State code for Colorado\n",
    "             'county':'005'} #County code for Arapahoe County\n",
    "    data = requests.get(api_url, params = param)\n",
    "    resolvedwo = data.json()\n",
    "    json_list.append(resolvedwo)\n",
    "\n",
    "with open('resolvedworesolution.json', 'w+') as f:\n",
    "    json.dump(json_list, f, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extact Values from the Json file\n",
    "The main data we want to track is the PM2.5 levels through time. We will need Date and Measurement of the PM2.5 levels    Regardless of where the key lives in the JSON, this extract_values function returns every value for the instance of \"key.\" \n",
    "\n",
    "*Source of function: https://hackersandslackers.com/extract-data-from-complex-json-python/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(obj, key):\n",
    "    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n",
    "    arr = []\n",
    "\n",
    "    def extract(obj, arr, key):\n",
    "        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                if isinstance(v, (dict, list)):\n",
    "                    extract(v, arr, key)\n",
    "                elif k == key:\n",
    "                    arr.append(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extract(item, arr, key)\n",
    "        return arr\n",
    "\n",
    "    results = extract(obj, arr, key)\n",
    "    return results\n",
    "\n",
    "local_date = extract_values(json_list, 'date_local')\n",
    "sample_measurement = extract_values(json_list, 'sample_measurement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe that has both the local date and sample measurement \n",
    "data = pd.DataFrame(list(zip(local_date, sample_measurement)),\n",
    "              columns=['local_date','sample_measurement'])\n",
    "data.local_date = pd.to_datetime(data.local_date, format=\"%Y-%m-%d\")\n",
    "#sort the dates in order\n",
    "data.sort_values(by=['local_date'], inplace=True, ascending=True)\n",
    "#the range of the PM2.5 through the last 10 years\n",
    "data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() #see how many non-null rows are in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "Looks like there are some large outliers and missing values in the dataframe. Max value in the PM2.5 samples is 140.4, which seems like an error. \n",
    "Also there are 123 NAN values. I am going to replace it with the median so that models do not have any errors in the time series analysis. Not going to use mean, as the data is right skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = data['sample_measurement'].median()\n",
    "#replace any outliers where it's greater than 100 to the median\n",
    "data.loc[data.sample_measurement > 100, 'sample_measurement'] = median\n",
    "#replace all NAN values with the median\n",
    "data['sample_measurement']= data['sample_measurement'].fillna(value = median)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As this is time series data, the date column will be set as the index\n",
    "data = data.set_index('local_date')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pyplot.rcParams[\"figure.figsize\"] = (20,10)\n",
    "pyplot.scatter(data.index, data.sample_measurement, marker = 'o')\n",
    "#adding a line at the 35PM2.5 levels, which the EPA deemed unsafe\n",
    "pyplot.axhline(y=35, color='r', linestyle='-')\n",
    "pyplot.legend([Line2D([0],[0], color = 'red', linewidth =3, linestyle = '-')], ['Dangerous Levels'], fontsize = 15)\n",
    "\n",
    "pyplot.rc('xtick', labelsize=15)    # fontsize of the tick labels\n",
    "pyplot.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
    "pyplot.title('PM2.5 Levels in Arapahoe County', fontsize=30)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are some days where the PM2.5 levels in Arapahoe County have exceeded safe levels. However, there seems to be no trend in PM2.5 levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = pd.DatetimeIndex(data.index).year\n",
    "data.head()\n",
    "data.boxplot(by ='year', column =['sample_measurement'], grid = False) \n",
    "pyplot.title('PM2.5 Levels in Arapahoe County by Year Boxplot', fontsize=30)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot of every year demonstrates that the median trend of the data has remained relatively flat. Also the number of outliers are also consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for Univariate Time Series Prediction\n",
    "\n",
    "I want to create a model that can predict the next 6 months of PM2.5 levels in the county. The sample is taken every 3 days, which would mean I would need to predict ~60 steps ahead. This is how I'll be training my model to see how well it does if I hold back 6 months of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('year', 1)\n",
    "steps = 60 #60 steps represents ~6 months of data. 60*3days = 180days\n",
    "train, test = data[:-steps], data[-steps:] \n",
    "\n",
    "'''\n",
    "Because the data is not trending up or down, and I have included outlier mix and max values (from boxplot), \n",
    "the MinMaxScaler should be an appropriate scaler. \n",
    "'''\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the LSTM model\n",
    "Going to create a tuning grid for the lstm model. The tuning parameters I want to adjust are the length param, number of starter nodes in the LSTM layer, dropout rate in the Dropout layer, and the number of epochs used to train the model. \n",
    "You can also change the activation and optimizer parameters to see if the tuning would work as well. \n",
    "\n",
    "*Note:* My compute could only handle ~150 iterations of the tuning grid. Therefore, in the first iteration, I have a diluted the parameters. If I was going to care about getting it closer, I would do this a couple of time, getting more narrow on the ranges for the best preformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_grid = {'n_input' : np.arange(2,63,60), #how many timesteps to use for prediction (for the length param in generator)\n",
    "             'batch_size' : np.arange(1,22,20),  #how samples will be taken from the train data (for the generator)\n",
    "             'nodes' : np.arange(50,1051,500), #how many intial nodes use for the LSTM layer\n",
    "            'dropout': np.arange(0,.5,.2), #what dropout rate to use for the Dropout layer for overfitting\n",
    "             'epochs' : np.arange(10,1011,1000)} #how many epochs to use in the fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = lstm_grid.keys()\n",
    "tuning_grid = pd.DataFrame(itertools.product(*[lstm_grid[k] for k in order]), columns=order)\n",
    "tuning_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(tuning_grid)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create an empty list for the RMSE errors that will come out of the prediction. I'm using RMSE here because this will compare the actual sample measurements to the predictions. Therefore, the RMSE will be easier to interpret. The list will be appended to the tuning grid dataframe to see which tuning parameter combinations resulted in the lowest RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse = [None]*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This for loop will go through each combination in the tuning grid. It will make a LSTM model, a prediction \n",
    "and calculate the RMSE between the actual data and the prediction. It will save off the RMSE where I can choose\n",
    "the best result (lowest). \n",
    "'''\n",
    "\n",
    "for i in range(m):\n",
    "    n_features = 1\n",
    "    tmp_n_input = tuning_grid.n_input[i]\n",
    "   #Time Series Generator\n",
    "    tmp_generator = TimeseriesGenerator(data = train, targets = train, \n",
    "                                        length = tmp_n_input, \n",
    "                                        batch_size = tuning_grid.batch_size[i])\n",
    "   #LSTM Model\n",
    "    tmp_model = Sequential()\n",
    "    tmp_model.add(LSTM(tuning_grid.nodes[i], \n",
    "                   activation = 'relu', \n",
    "                   input_shape =(tmp_n_input, n_features)))\n",
    "    tmp_model.add(Dropout(tuning_grid.dropout[i]))\n",
    "    tmp_model.add(Dense(1))\n",
    "    tmp_model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "    tmp_model.fit_generator(tmp_generator, steps_per_epoch =1, epochs=tuning_grid.dropout[i])\n",
    "\n",
    "    #Prediction\n",
    "    tmp_x_input = train[-tmp_n_input:].reshape((1, tmp_n_input, n_features))\n",
    "\n",
    "    tmp_pred_list = []\n",
    "\n",
    "    for j in range(steps):\n",
    "        #one step ahead prediciton\n",
    "        tmp_pred_list.append(tmp_model.predict(tmp_x_input)[0]) \n",
    "        #append the one-step prediction to the x_input used to forecast the following step\n",
    "        tmp_x_input = np.append(tmp_x_input[:, 1:, :], [[tmp_pred_list[j]]], axis = 1) \n",
    "\n",
    "    tmp_df_predict = pd.DataFrame(scaler.inverse_transform(tmp_pred_list), \n",
    "                                  index = data[-steps:].index, columns = [\"tmp_predictions\"])\n",
    "    tmp_df_test = pd.concat([data, tmp_df_predict], axis = 1)\n",
    "\n",
    "    lstm_rmse[i] = sqrt(mean_squared_error(tmp_df_test.sample_measurement[-steps:], \n",
    "                                           tmp_df_test.tmp_predictions[-steps:]))\n",
    "\n",
    "    #To track the progress \n",
    "    print(i,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = pd.concat([tuning_grid, pd.DataFrame(lstm_rmse, columns = ['lstm_rmse'])], axis = 1)\n",
    "tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see which combination had the lowest rmse\n",
    "tuning_results[['lstm_rmse']].idxmin() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best results were at index 70\n",
    "tuning_results.loc[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_index = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final LSTM model based off the best preforming tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "n_input = tuning_grid.n_input[best_result_index]\n",
    "#Time Series Generator\n",
    "generator = TimeseriesGenerator(data = train, targets = train, \n",
    "                                length = n_input, \n",
    "                                batch_size = tuning_grid.batch_size[best_result_index])\n",
    "#LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(tuning_grid.nodes[best_result_index], \n",
    "                   activation = 'relu', \n",
    "                   input_shape =(n_input, n_features)))\n",
    "model.add(Dropout(tuning_grid.dropout[best_result_index]))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "model.fit_generator(generator, steps_per_epoch =1, epochs=tuning_grid.dropout[best_result_index])\n",
    "\n",
    "#Prediction\n",
    "x_input = train[-n_input:].reshape((1, n_input, n_features))\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "for j in range(steps):\n",
    "    #one step ahead prediciton\n",
    "    pred_list.append(model.predict(x_input)[0]) \n",
    "    #append the one-step prediction to the x_input used to forecast the following step\n",
    "    x_input = np.append(x_input[:, 1:, :], [[pred_list[j]]], axis = 1) \n",
    "\n",
    "df_predict = pd.DataFrame(scaler.inverse_transform(pred_list), index = data[-steps:].index, columns = [\"predictions\"])\n",
    "\n",
    "df_test = pd.concat([data, df_predict], axis = 1)\n",
    "\n",
    "lstm_rmse = sqrt(mean_squared_error(df_test.sample_measurement[-steps:], df_test.predictions[-steps:]))\n",
    "print('The rmse is', lstm_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(df_test.index[-(steps+60):], df_test.sample_measurement[-(steps+60):])\n",
    "pyplot.plot(df_test.index[-(steps+60):], df_test.predictions[-(steps+60):], color = 'r')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the RMSE between the prediction and actual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make training data the whole dataframe\n",
    "future_predict_train = data\n",
    "\n",
    "scaler.fit(future_predict_train)\n",
    "future_predict_train = scaler.transform(future_predict_train)\n",
    "\n",
    "future_predict_generator = TimeseriesGenerator(data = future_predict_train, targets = future_predict_train, \n",
    "                                length = n_input, batch_size = tuning_grid.batch_size[best_result_index])\n",
    "\n",
    "#using the same model as above\n",
    "model.fit_generator(future_predict_generator, steps_per_epoch = 1, epochs= tuning_grid.dropout[best_result_index])\n",
    "\n",
    "future_x_input = future_predict_train[-n_input:].reshape((1, n_input, n_features))\n",
    "\n",
    "future_predict_pred_list = []\n",
    "\n",
    "for i in range(steps):\n",
    "    #one step ahead prediciton\n",
    "    future_predict_pred_list.append(model.predict(future_x_input)[0]) \n",
    "    #append the one-step prediction to the x_input used to forecast the following step\n",
    "    future_x_input = np.append(future_x_input[:, 1:, :], [[future_predict_pred_list[i]]], axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding future dates for the prediction. Because the sample is taken every 3 days, the range has to be increased by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dates = [data.index[-1] + datetime.timedelta(days=x) for x in range(0,steps*3+1,3)]\n",
    "future_dates = pd.DataFrame(index = add_dates[1:], columns = data.columns)\n",
    "future_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_future_predict = pd.DataFrame(scaler.inverse_transform(pred_list),\n",
    "                                index= future_dates[-steps:].index, columns = ['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj = pd.concat([data, df_future_predict], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(df_proj.index[-(steps+60):], df_proj.sample_measurement[-(steps+60):])\n",
    "pyplot.plot(df_proj.index[-(steps+60):], df_proj.prediction[-(steps+60):], color = 'r')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Model for comparison \n",
    "Usuallly ARIMA can model univariate time series models quiet well. Interested in seeing how well it preforms against LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 60 #going to keep the same amount of steps for comparison \n",
    "train, test = data[:-steps], data[-steps:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "\n",
    "model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "model.fit(train)\n",
    "\n",
    "forecast = model.predict(n_periods=len(test))\n",
    "forecast = pd.DataFrame(forecast ,index = test.index,columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj = pd.concat([data, forecast], axis = 1)\n",
    "df_proj.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = sqrt(mean_squared_error(test,forecast))\n",
    "print(\"ARMIA's RMSE is:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(train[-(steps+60):], label='Train')\n",
    "pyplot.plot(test, label='Test')\n",
    "pyplot.plot(forecast, label='Prediction')\n",
    "pyplot.legend(fontsize = 15)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
